{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ELECTRA Fine-Tuned + GLoVe Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 0. Taking a Look at the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id keyword                        location  \\\n",
       "2010  2886  damage                             NaN   \n",
       "2011  2887  damage                             NaN   \n",
       "2012  2888  damage                             NaN   \n",
       "2013  2889  damage                    Charlotte NC   \n",
       "2014  2890  damage                             NaN   \n",
       "2015  2891  damage             Rockville, Maryland   \n",
       "2016  2893  damage                             NaN   \n",
       "2017  2895  damage                             NaN   \n",
       "2018  2896  damage                       Australia   \n",
       "2019  2898  damage               Your Conversation   \n",
       "2020  2899  damage                           USAoV   \n",
       "2021  2900  damage  Bhopal, Madhya Pradesh, India.   \n",
       "2022  2902  damage                       Tennessee   \n",
       "2023  2905  danger                             NaN   \n",
       "2024  2907  danger                     #LemonGang    \n",
       "2025  2908  danger                             ayr   \n",
       "2026  2909  danger                             NaN   \n",
       "2027  2910  danger                       Worldwide   \n",
       "2028  2911  danger            San Jose, California   \n",
       "2029  2912  danger                       Worldwide   \n",
       "\n",
       "                                                   text  target  \n",
       "2010                        Beach did damage to my shit       1  \n",
       "2011  @WonderousAllure crosses her arms to cover her...       0  \n",
       "2012  New post on my blog: http://t.co/Avu9b4k2rv \\n...       0  \n",
       "2013  REPORTED: HIT &amp; RUN-IN ROADWAY-PROPERTY DA...       1  \n",
       "2014  Devil May Cry 4 Special Edition Vergil Vs Agnu...       0  \n",
       "2015  #Glaucoma occurs when fluid builds up pressure...       1  \n",
       "2016  #JSunNews Storm damage reported in Madison Cou...       1  \n",
       "2017  S61.231A Puncture wound without foreign body o...       1  \n",
       "2018  Thank you @RicharkKirkArch @AusInstArchitect f...       0  \n",
       "2019                 This real shit will damage a bitch       0  \n",
       "2020  lmao fuckboy changed his @ for damage control\\...       0  \n",
       "2021            @MichaelWestBiz standard damage control       1  \n",
       "2022  @GettingLost @JennEllensBB @Muncle_jim It said...       1  \n",
       "2023  @BlizzHeroes @DustinBrowder DAD. I won't chase...       0  \n",
       "2024  I believe there is a shirt company now for eve...       0  \n",
       "2025       Danger of union bears http://t.co/lhdcpNZx6A       0  \n",
       "2026  The sign-up is open for the FALLING FOR DANGER...       1  \n",
       "2027  @DyannBridges @yeshayad Check out this #rockin...       0  \n",
       "2028  Red Flag Warning for fire danger &amp; dry thu...       1  \n",
       "2029  @CarsonRex @SpaceAngelSeven Check out this #ro...       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010</th>\n      <td>2886</td>\n      <td>damage</td>\n      <td>NaN</td>\n      <td>Beach did damage to my shit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>2887</td>\n      <td>damage</td>\n      <td>NaN</td>\n      <td>@WonderousAllure crosses her arms to cover her...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2012</th>\n      <td>2888</td>\n      <td>damage</td>\n      <td>NaN</td>\n      <td>New post on my blog: http://t.co/Avu9b4k2rv \\n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013</th>\n      <td>2889</td>\n      <td>damage</td>\n      <td>Charlotte NC</td>\n      <td>REPORTED: HIT &amp;amp; RUN-IN ROADWAY-PROPERTY DA...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014</th>\n      <td>2890</td>\n      <td>damage</td>\n      <td>NaN</td>\n      <td>Devil May Cry 4 Special Edition Vergil Vs Agnu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>2891</td>\n      <td>damage</td>\n      <td>Rockville, Maryland</td>\n      <td>#Glaucoma occurs when fluid builds up pressure...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>2893</td>\n      <td>damage</td>\n      <td>NaN</td>\n      <td>#JSunNews Storm damage reported in Madison Cou...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>2895</td>\n      <td>damage</td>\n      <td>NaN</td>\n      <td>S61.231A Puncture wound without foreign body o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>2896</td>\n      <td>damage</td>\n      <td>Australia</td>\n      <td>Thank you @RicharkKirkArch @AusInstArchitect f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>2898</td>\n      <td>damage</td>\n      <td>Your Conversation</td>\n      <td>This real shit will damage a bitch</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>2899</td>\n      <td>damage</td>\n      <td>USAoV</td>\n      <td>lmao fuckboy changed his @ for damage control\\...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>2900</td>\n      <td>damage</td>\n      <td>Bhopal, Madhya Pradesh, India.</td>\n      <td>@MichaelWestBiz standard damage control</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>2902</td>\n      <td>damage</td>\n      <td>Tennessee</td>\n      <td>@GettingLost @JennEllensBB @Muncle_jim It said...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023</th>\n      <td>2905</td>\n      <td>danger</td>\n      <td>NaN</td>\n      <td>@BlizzHeroes @DustinBrowder DAD. I won't chase...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2024</th>\n      <td>2907</td>\n      <td>danger</td>\n      <td>#LemonGang</td>\n      <td>I believe there is a shirt company now for eve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2025</th>\n      <td>2908</td>\n      <td>danger</td>\n      <td>ayr</td>\n      <td>Danger of union bears http://t.co/lhdcpNZx6A</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2026</th>\n      <td>2909</td>\n      <td>danger</td>\n      <td>NaN</td>\n      <td>The sign-up is open for the FALLING FOR DANGER...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2027</th>\n      <td>2910</td>\n      <td>danger</td>\n      <td>Worldwide</td>\n      <td>@DyannBridges @yeshayad Check out this #rockin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2028</th>\n      <td>2911</td>\n      <td>danger</td>\n      <td>San Jose, California</td>\n      <td>Red Flag Warning for fire danger &amp;amp; dry thu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2029</th>\n      <td>2912</td>\n      <td>danger</td>\n      <td>Worldwide</td>\n      <td>@CarsonRex @SpaceAngelSeven Check out this #ro...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data[2010:].head(20)"
   ]
  },
  {
   "source": [
    "---\n",
    "### NOTE\n",
    "By looking at the data, we can see the *location* data is actually very useful,  \n",
    "because a serious Tweet will generally have a proper location,      \n",
    "whereas a jokeful Tweet will have something silly.  \n",
    "So let's keep the *location* data.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id keyword                        location  \\\n",
       "2010  2886  damage                                   \n",
       "2011  2887  damage                                   \n",
       "2012  2888  damage                                   \n",
       "2013  2889  damage                    charlotte nc   \n",
       "2014  2890  damage                                   \n",
       "2015  2891  damage             rockville  maryland   \n",
       "2016  2893  damage                                   \n",
       "2017  2895  damage                                   \n",
       "2018  2896  damage                       australia   \n",
       "2019  2898  damage               your conversation   \n",
       "2020  2899  damage                           usaov   \n",
       "2021  2900  damage  bhopal  madhya pradesh  india    \n",
       "2022  2902  damage                       tennessee   \n",
       "2023  2905  danger                                   \n",
       "2024  2907  danger                      lemongang    \n",
       "2025  2908  danger                             ayr   \n",
       "2026  2909  danger                                   \n",
       "2027  2910  danger                       worldwide   \n",
       "2028  2911  danger            san jose  california   \n",
       "2029  2912  danger                       worldwide   \n",
       "\n",
       "                                                   text  target  \n",
       "2010                        beach did damage to my shit       1  \n",
       "2011   wonderousallure crosses her arms to cover her...       0  \n",
       "2012  new post on my blog  url  thesensualeye   mode...       0  \n",
       "2013  reported  hit  run in roadway property damage ...       1  \n",
       "2014  devil may cry 0 special edition vergil vs agnu...       0  \n",
       "2015   glaucoma occurs when fluid builds up pressure...       1  \n",
       "2016   jsunnews storm damage reported in madison cou...       1  \n",
       "2017  s00 000a puncture wound without foreign body o...       1  \n",
       "2018  thank you  richarkkirkarch  ausinstarchitect f...       0  \n",
       "2019                 this real shit will damage a bitch       0  \n",
       "2020  lmao fuckboy changed his   for damage control ...       0  \n",
       "2021             michaelwestbiz standard damage control       1  \n",
       "2022   gettinglost  jennellensbb  muncle jim it said...       1  \n",
       "2023   blizzheroes  dustinbrowder dad  i won t chase...       0  \n",
       "2024  i believe there is a shirt company now for eve...       0  \n",
       "2025                          danger of union bears url       0  \n",
       "2026  the sign up is open for the falling for danger...       1  \n",
       "2027   dyannbridges  yeshayad check out this  rockin...       0  \n",
       "2028  red flag warning for fire danger  dry thunders...       1  \n",
       "2029   carsonrex  spaceangelseven check out this  ro...       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010</th>\n      <td>2886</td>\n      <td>damage</td>\n      <td></td>\n      <td>beach did damage to my shit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>2887</td>\n      <td>damage</td>\n      <td></td>\n      <td>wonderousallure crosses her arms to cover her...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2012</th>\n      <td>2888</td>\n      <td>damage</td>\n      <td></td>\n      <td>new post on my blog  url  thesensualeye   mode...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013</th>\n      <td>2889</td>\n      <td>damage</td>\n      <td>charlotte nc</td>\n      <td>reported  hit  run in roadway property damage ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014</th>\n      <td>2890</td>\n      <td>damage</td>\n      <td></td>\n      <td>devil may cry 0 special edition vergil vs agnu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>2891</td>\n      <td>damage</td>\n      <td>rockville  maryland</td>\n      <td>glaucoma occurs when fluid builds up pressure...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>2893</td>\n      <td>damage</td>\n      <td></td>\n      <td>jsunnews storm damage reported in madison cou...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>2895</td>\n      <td>damage</td>\n      <td></td>\n      <td>s00 000a puncture wound without foreign body o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>2896</td>\n      <td>damage</td>\n      <td>australia</td>\n      <td>thank you  richarkkirkarch  ausinstarchitect f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>2898</td>\n      <td>damage</td>\n      <td>your conversation</td>\n      <td>this real shit will damage a bitch</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>2899</td>\n      <td>damage</td>\n      <td>usaov</td>\n      <td>lmao fuckboy changed his   for damage control ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>2900</td>\n      <td>damage</td>\n      <td>bhopal  madhya pradesh  india</td>\n      <td>michaelwestbiz standard damage control</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>2902</td>\n      <td>damage</td>\n      <td>tennessee</td>\n      <td>gettinglost  jennellensbb  muncle jim it said...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023</th>\n      <td>2905</td>\n      <td>danger</td>\n      <td></td>\n      <td>blizzheroes  dustinbrowder dad  i won t chase...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2024</th>\n      <td>2907</td>\n      <td>danger</td>\n      <td>lemongang</td>\n      <td>i believe there is a shirt company now for eve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2025</th>\n      <td>2908</td>\n      <td>danger</td>\n      <td>ayr</td>\n      <td>danger of union bears url</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2026</th>\n      <td>2909</td>\n      <td>danger</td>\n      <td></td>\n      <td>the sign up is open for the falling for danger...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2027</th>\n      <td>2910</td>\n      <td>danger</td>\n      <td>worldwide</td>\n      <td>dyannbridges  yeshayad check out this  rockin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2028</th>\n      <td>2911</td>\n      <td>danger</td>\n      <td>san jose  california</td>\n      <td>red flag warning for fire danger  dry thunders...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2029</th>\n      <td>2912</td>\n      <td>danger</td>\n      <td>worldwide</td>\n      <td>carsonrex  spaceangelseven check out this  ro...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# let's apply regex to clean the strings\n",
    "\n",
    "data['text'] = data['text'].str.replace('http\\S+', 'url', regex=True)  # replace all URLs with 'url'\n",
    "data['text'] = data['text'].str.replace('&\\S+', '', regex=True)  # remove all html junks ex) &amp;\n",
    "data['text'] = data['text'].str.replace('[0-9]','0', regex=True)  # replace all integer values with 0\n",
    "data['text'] = data['text'].str.replace('[^a-zA-Z0 ]', ' ', regex=True)  # replace all non-numerics, non-alphabets with space\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "data['location'] = data['location'].str.replace('http\\S+', 'http', regex=True)\n",
    "data['location'] = data['location'].str.replace('&\\S+', '', regex=True)\n",
    "data['location'] = data['location'].str.replace('[0-9]','0', regex=True)\n",
    "data['location'] = data['location'].str.replace('[^a-zA-Z0 ]', ' ', regex=True)\n",
    "data['location'] = data['location'].str.lower()\n",
    "data['location'] = data['location'].fillna('')  # replace NaN values with empty string\n",
    "\n",
    "data[2010:].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Kevin drink', 'Yoon life again'], dtype='<U15')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(pd_series): # lemmatize words to a more general term\n",
    "    ret = []\n",
    "    for entry in pd_series:\n",
    "        sent = ''\n",
    "        words = entry.split()\n",
    "        for word in words:\n",
    "            sent = sent + lemm.lemmatize(word) + ' '\n",
    "        ret.append(sent[:-1])\n",
    "    return np.asarray(ret)\n",
    "\n",
    "lemmatize(['Kevin drinks', 'Yoon lives again'])  # some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id keyword                     location  \\\n",
       "2010  2886  damage                                \n",
       "2011  2887  damage                                \n",
       "2012  2888  damage                                \n",
       "2013  2889  damage                 charlotte nc   \n",
       "2014  2890  damage                                \n",
       "2015  2891  damage           rockville maryland   \n",
       "2016  2893  damage                                \n",
       "2017  2895  damage                                \n",
       "2018  2896  damage                    australia   \n",
       "2019  2898  damage            your conversation   \n",
       "2020  2899  damage                        usaov   \n",
       "2021  2900  damage  bhopal madhya pradesh india   \n",
       "2022  2902  damage                    tennessee   \n",
       "2023  2905  danger                                \n",
       "2024  2907  danger                    lemongang   \n",
       "2025  2908  danger                          ayr   \n",
       "2026  2909  danger                                \n",
       "2027  2910  danger                    worldwide   \n",
       "2028  2911  danger          san jose california   \n",
       "2029  2912  danger                    worldwide   \n",
       "\n",
       "                                                   text  target  \n",
       "2010                        beach did damage to my shit       1  \n",
       "2011  wonderousallure cross her arm to cover her han...       0  \n",
       "2012  new post on my blog url thesensualeye model ca...       0  \n",
       "2013  reported hit run in roadway property damage at...       1  \n",
       "2014  devil may cry 0 special edition vergil v agnus...       0  \n",
       "2015  glaucoma occurs when fluid build up pressure i...       1  \n",
       "2016  jsunnews storm damage reported in madison coun...       1  \n",
       "2017  s00 000a puncture wound without foreign body o...       1  \n",
       "2018  thank you richarkkirkarch ausinstarchitect for...       0  \n",
       "2019                 this real shit will damage a bitch       0  \n",
       "2020  lmao fuckboy changed his for damage control ps...       0  \n",
       "2021             michaelwestbiz standard damage control       1  \n",
       "2022  gettinglost jennellensbb muncle jim it said th...       1  \n",
       "2023  blizzheroes dustinbrowder dad i won t chase yo...       0  \n",
       "2024  i believe there is a shirt company now for eve...       0  \n",
       "2025                           danger of union bear url       0  \n",
       "2026  the sign up is open for the falling for danger...       1  \n",
       "2027  dyannbridges yeshayad check out this rockin pr...       0  \n",
       "2028  red flag warning for fire danger dry thunderst...       1  \n",
       "2029  carsonrex spaceangelseven check out this rocki...       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010</th>\n      <td>2886</td>\n      <td>damage</td>\n      <td></td>\n      <td>beach did damage to my shit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>2887</td>\n      <td>damage</td>\n      <td></td>\n      <td>wonderousallure cross her arm to cover her han...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2012</th>\n      <td>2888</td>\n      <td>damage</td>\n      <td></td>\n      <td>new post on my blog url thesensualeye model ca...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013</th>\n      <td>2889</td>\n      <td>damage</td>\n      <td>charlotte nc</td>\n      <td>reported hit run in roadway property damage at...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014</th>\n      <td>2890</td>\n      <td>damage</td>\n      <td></td>\n      <td>devil may cry 0 special edition vergil v agnus...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>2891</td>\n      <td>damage</td>\n      <td>rockville maryland</td>\n      <td>glaucoma occurs when fluid build up pressure i...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>2893</td>\n      <td>damage</td>\n      <td></td>\n      <td>jsunnews storm damage reported in madison coun...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>2895</td>\n      <td>damage</td>\n      <td></td>\n      <td>s00 000a puncture wound without foreign body o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>2896</td>\n      <td>damage</td>\n      <td>australia</td>\n      <td>thank you richarkkirkarch ausinstarchitect for...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>2898</td>\n      <td>damage</td>\n      <td>your conversation</td>\n      <td>this real shit will damage a bitch</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>2899</td>\n      <td>damage</td>\n      <td>usaov</td>\n      <td>lmao fuckboy changed his for damage control ps...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>2900</td>\n      <td>damage</td>\n      <td>bhopal madhya pradesh india</td>\n      <td>michaelwestbiz standard damage control</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>2902</td>\n      <td>damage</td>\n      <td>tennessee</td>\n      <td>gettinglost jennellensbb muncle jim it said th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023</th>\n      <td>2905</td>\n      <td>danger</td>\n      <td></td>\n      <td>blizzheroes dustinbrowder dad i won t chase yo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2024</th>\n      <td>2907</td>\n      <td>danger</td>\n      <td>lemongang</td>\n      <td>i believe there is a shirt company now for eve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2025</th>\n      <td>2908</td>\n      <td>danger</td>\n      <td>ayr</td>\n      <td>danger of union bear url</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2026</th>\n      <td>2909</td>\n      <td>danger</td>\n      <td></td>\n      <td>the sign up is open for the falling for danger...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2027</th>\n      <td>2910</td>\n      <td>danger</td>\n      <td>worldwide</td>\n      <td>dyannbridges yeshayad check out this rockin pr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2028</th>\n      <td>2911</td>\n      <td>danger</td>\n      <td>san jose california</td>\n      <td>red flag warning for fire danger dry thunderst...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2029</th>\n      <td>2912</td>\n      <td>danger</td>\n      <td>worldwide</td>\n      <td>carsonrex spaceangelseven check out this rocki...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data['text'] = lemmatize(data['text'])\n",
    "data['location'] = lemmatize(data['location'])\n",
    "data[2010:].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id      keyword                    location  \\\n",
       "2010   8431    sandstorm                         usa   \n",
       "2011   1485  body%20bags  westside of philly 0 block   \n",
       "2012    704     attacked       0 0 of the blam squad   \n",
       "2013   5580        flood                    new york   \n",
       "2014   3957  devastation             newport wale uk   \n",
       "2015   7694        panic  torry alvarez love forever   \n",
       "2016  10731        wreck                   canada bc   \n",
       "2017   5035   eyewitness                       india   \n",
       "2018  10260   war%20zone                               \n",
       "2019   9600      thunder                    macon ga   \n",
       "2020   3465     derailed      the desert of the real   \n",
       "2021    856    bioterror                           u   \n",
       "2022   9542       threat                  everywhere   \n",
       "2023   6363     hostages            pune maharashtra   \n",
       "2024    337  annihilated       university of toronto   \n",
       "2025    986      blazing                               \n",
       "2026    961        blaze                       delhi   \n",
       "2027    563        arson                milwaukee wi   \n",
       "2028   8283      rioting          colonial height va   \n",
       "2029   5907         harm                               \n",
       "\n",
       "                                                   text  target  \n",
       "2010  watch this airport get swallowed up by a sands...       1  \n",
       "2011              ain t no bag in the trunk it s a body       0  \n",
       "2012                        i m feeling so attacked url       0  \n",
       "2013  0pcs 00w cree led work light offroad lamp car ...       1  \n",
       "2014  cllrraymogford indeed ray devastation would be...       1  \n",
       "2015                          panic at the disco te amo       0  \n",
       "2016          raineishida lol im just a nervous wreck p       0  \n",
       "2017  read a schoolboy s eyewitness account of hiros...       1  \n",
       "2018  they turned jasmine house into a war zone litt...       0  \n",
       "2019       thunder outside my house this afternoon gawx       1  \n",
       "2020  trustymclusty no passenger were on the deraile...       1  \n",
       "2021  fedex no longer to transport bioterror germ in...       1  \n",
       "2022  build and share your own custom application al...       0  \n",
       "2023   minhazmerchant great job done by village hostage       1  \n",
       "2024  sirbrandonknt exactly that s why the lesnar ce...       0  \n",
       "2025  this bowl got me thinking damn i ve been blazi...       0  \n",
       "2026  socialmedia news new facebook page feature see...       0  \n",
       "2027  owner of chicago area gay bar admits to arson ...       0  \n",
       "2028  halljh0000 i am so sick of criminal parent and...       1  \n",
       "2029  angel star00 obama should feel responsible bri...       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2010</th>\n      <td>8431</td>\n      <td>sandstorm</td>\n      <td>usa</td>\n      <td>watch this airport get swallowed up by a sands...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2011</th>\n      <td>1485</td>\n      <td>body%20bags</td>\n      <td>westside of philly 0 block</td>\n      <td>ain t no bag in the trunk it s a body</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2012</th>\n      <td>704</td>\n      <td>attacked</td>\n      <td>0 0 of the blam squad</td>\n      <td>i m feeling so attacked url</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2013</th>\n      <td>5580</td>\n      <td>flood</td>\n      <td>new york</td>\n      <td>0pcs 00w cree led work light offroad lamp car ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2014</th>\n      <td>3957</td>\n      <td>devastation</td>\n      <td>newport wale uk</td>\n      <td>cllrraymogford indeed ray devastation would be...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2015</th>\n      <td>7694</td>\n      <td>panic</td>\n      <td>torry alvarez love forever</td>\n      <td>panic at the disco te amo</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016</th>\n      <td>10731</td>\n      <td>wreck</td>\n      <td>canada bc</td>\n      <td>raineishida lol im just a nervous wreck p</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2017</th>\n      <td>5035</td>\n      <td>eyewitness</td>\n      <td>india</td>\n      <td>read a schoolboy s eyewitness account of hiros...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2018</th>\n      <td>10260</td>\n      <td>war%20zone</td>\n      <td></td>\n      <td>they turned jasmine house into a war zone litt...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>9600</td>\n      <td>thunder</td>\n      <td>macon ga</td>\n      <td>thunder outside my house this afternoon gawx</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>3465</td>\n      <td>derailed</td>\n      <td>the desert of the real</td>\n      <td>trustymclusty no passenger were on the deraile...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>856</td>\n      <td>bioterror</td>\n      <td>u</td>\n      <td>fedex no longer to transport bioterror germ in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>9542</td>\n      <td>threat</td>\n      <td>everywhere</td>\n      <td>build and share your own custom application al...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2023</th>\n      <td>6363</td>\n      <td>hostages</td>\n      <td>pune maharashtra</td>\n      <td>minhazmerchant great job done by village hostage</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2024</th>\n      <td>337</td>\n      <td>annihilated</td>\n      <td>university of toronto</td>\n      <td>sirbrandonknt exactly that s why the lesnar ce...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2025</th>\n      <td>986</td>\n      <td>blazing</td>\n      <td></td>\n      <td>this bowl got me thinking damn i ve been blazi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2026</th>\n      <td>961</td>\n      <td>blaze</td>\n      <td>delhi</td>\n      <td>socialmedia news new facebook page feature see...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2027</th>\n      <td>563</td>\n      <td>arson</td>\n      <td>milwaukee wi</td>\n      <td>owner of chicago area gay bar admits to arson ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2028</th>\n      <td>8283</td>\n      <td>rioting</td>\n      <td>colonial height va</td>\n      <td>halljh0000 i am so sick of criminal parent and...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2029</th>\n      <td>5907</td>\n      <td>harm</td>\n      <td></td>\n      <td>angel star00 obama should feel responsible bri...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# shuffle the data, make sure to set the random_state(the seed for shuffling) to a fixed integer,\n",
    "# so the validation set can have the same data for proper validation every time you run the code. \n",
    "data = data.sample(frac=1, random_state=1).reset_index(drop=True)  \n",
    "data[2010:].head(20)"
   ]
  },
  {
   "source": [
    "## 2. Tokenizing and Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "### NOTE\n",
    "We are going to tokenize the main text data with ELECTRA Tokenzier, so we can actually feed the data into ELECTRA.  \n",
    "We are only going to use GLoVe embeddings on location texts, because they are shorter, and the sequence of the words are less important.  \n",
    "i.e. we only need to know if the words are actually describing a real location or not."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from transformers import ElectraTokenizer\n",
    "# Huggingface Transformers ELECTRA: https://huggingface.co/transformers/model_doc/electra.html\n",
    "\n",
    "# tokenize the main text data with ElectraTokenizer\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "texts = data['text'].tolist()\n",
    "texts = tokenizer(texts, truncation=True, padding=True)  # pads to longest sentence, but truncates if it exceeds max_len of the specific model, in ELECTRA's case 512\n",
    "\n",
    "token_input = np.asarray(texts['input_ids'])  # sentence with words converted to token\n",
    "mask_input = np.asarray(texts['attention_mask'])  # masking: if token is <pad> then 0, else 1\n",
    "\n",
    "print(token_input.shape)  # in our case 53 is the longest sentence\n",
    "print(mask_input.shape)\n",
    "\n",
    "emb_len = token_input.shape[-1]  # save 53 for future use"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7613, 53)\n(7613, 53)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('glove.pickle', 'rb') as p:\n",
    "    glove_dict = pickle.load(p)  # dictionary containing pre-trained GLoVe, with 100 dimension vector representing a word\n",
    "\n",
    "# Stanford GLoVe : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "def gloveAndPad(pd_series, max_length):  # converts words in sentence to GLoVe vectors, and pads to meet the max_length, truncs if longer\n",
    "    ret = []\n",
    "    for entry in pd_series:\n",
    "        emb_sent = []\n",
    "        words = entry.split()\n",
    "        for word in words:\n",
    "            try:  # if the word is in the GLoVe data\n",
    "                emb_sent.append(glove_dict[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        while len(emb_sent) < max_length:  # pad to max_length\n",
    "            emb_sent.append(np.zeros(100, dtype='float32'))\n",
    "        ret.append(np.asarray(emb_sent)[:max_length])  # trunc to max_length\n",
    "    return np.asarray(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'loc_max_len': 5,  # I picked 5 as max length for location but you can try out different lengths\n",
    "    'glove_emb_dim' : 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7613, 5, 100)\n(7613,)\n"
     ]
    }
   ],
   "source": [
    "loc_input = gloveAndPad(data['location'], config['loc_max_len'])\n",
    "\n",
    "labels = np.asarray(data['target']) # prepare the label as well\n",
    "\n",
    "print(loc_input.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "source": [
    "## 3. Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error() \n",
    "# this removes warnings from the transformers module, which occurs because we are loading the discriminator weights on a ordinary Model class,\n",
    "# and therefore some of the weights that are only used during pre-training the discriminator doesn't have a place to load.\n",
    "# However, this is a normal behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ntokens (InputLayer)             [(None, 53)]         0                                            \n__________________________________________________________________________________________________\nmasks (InputLayer)              [(None, 53)]         0                                            \n__________________________________________________________________________________________________\ntf_electra_model (TFElectraMode TFBaseModelOutput(la 108891648   tokens[0][0]                     \n                                                                 masks[0][0]                      \n__________________________________________________________________________________________________\nlocs (InputLayer)               [(None, 5, 100)]     0                                            \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 768)          0           tf_electra_model[0][0]           \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 100)          0           locs[0][0]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           3232        global_average_pooling1d_1[0][0] \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndropout_38 (Dropout)            (None, 32)           0           dense_1[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 288)          0           dropout_37[0][0]                 \n                                                                 dropout_38[0][0]                 \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 64)           18496       concatenate[0][0]                \n__________________________________________________________________________________________________\ndropout_39 (Dropout)            (None, 64)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 16)           1040        dropout_39[0][0]                 \n__________________________________________________________________________________________________\ndropout_40 (Dropout)            (None, 16)           0           dense_3[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 1)            17          dropout_40[0][0]                 \n==================================================================================================\nTotal params: 109,111,297\nTrainable params: 109,111,297\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFElectraModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# three inputs\n",
    "input_tokens = tf.keras.layers.Input(shape=(emb_len, ), name='tokens', dtype='int32')\n",
    "input_masks = tf.keras.layers.Input(shape=(emb_len, ), name='masks', dtype='int32')\n",
    "input_locs = tf.keras.layers.Input(shape=(config['loc_max_len'], config['glove_emb_dim']), name='locs', dtype='float32')\n",
    "\n",
    "electra_model = TFElectraModel.from_pretrained('google/electra-base-discriminator')\n",
    "electra_output = electra_model(input_tokens, attention_mask=input_masks).last_hidden_state  # we only need the last hidden outputs\n",
    "\n",
    "x1 = tf.keras.layers.GlobalAveragePooling1D()(electra_output)\n",
    "x1 = tf.keras.layers.Dense(256, activation='relu')(x1)\n",
    "x1 = tf.keras.layers.Dropout(0.2)(x1)\n",
    "\n",
    "x2 = tf.keras.layers.GlobalAveragePooling1D()(input_locs)\n",
    "x2 = tf.keras.layers.Dense(32, activation='relu')(x2)\n",
    "x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "\n",
    "x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "y = tf.keras.layers.Dense(1, activation='sigmoid')(x) # output\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_tokens, input_masks, input_locs], outputs=y)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=2e-5)  # choose a low learning rate to fine-tune, a high learning rate will disrupt all the pre-trained weights\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## 4. Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "476/476 [==============================] - 93s 195ms/step - loss: 0.4850 - accuracy: 0.7829\n",
      "Epoch 2/2\n",
      "476/476 [==============================] - 87s 184ms/step - loss: 0.3828 - accuracy: 0.8518\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2036d1c46a0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#model.fit(x={'tokens':token_input, 'masks':mask_input, 'locs':loc_input}, y=labels, epochs=10, batch_size=16, validation_split=0.2) # for validation\n",
    "model.fit(x={'tokens':token_input, 'masks':mask_input, 'locs':loc_input}, y=labels, epochs=2, batch_size=16) # for actual training"
   ]
  },
  {
   "source": [
    "---\n",
    "### NOTE\n",
    "You can test out different numbers of epochs and check the validaiton loss, in my case 2 epoch was the sweetspot.   \n",
    "Generally fewer epochs will work well to fine-tune a transformer model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5. Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3263, 53)\n(3263, 53)\n(3263, 5, 100)\n"
     ]
    }
   ],
   "source": [
    "# Apply Preprocssing to Test Data\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "test_data['text'] = test_data['text'].str.replace('http\\S+', 'http', regex=True)\n",
    "test_data['text'] = test_data['text'].str.replace('&\\S+', '', regex=True)\n",
    "test_data['text'] = test_data['text'].str.replace('[0-9]','0', regex=True)\n",
    "test_data['text'] = test_data['text'].str.replace('[^a-zA-Z0 ]', ' ', regex=True)\n",
    "test_data['text'] = test_data['text'].str.lower()\n",
    "\n",
    "test_data['location'] = test_data['location'].str.replace('http\\S+', 'http', regex=True) \n",
    "test_data['location'] = test_data['location'].str.replace('&\\S+', '', regex=True)  \n",
    "test_data['location'] = test_data['location'].str.replace('[0-9]','0', regex=True)  \n",
    "test_data['location'] = test_data['location'].str.replace('[^a-zA-Z0 ]', ' ', regex=True)\n",
    "test_data['location'] = test_data['location'].str.lower()\n",
    "test_data['location'] = test_data['location'].fillna('')\n",
    "\n",
    "test_texts = test_data['text'].tolist()\n",
    "\n",
    "test_texts = tokenizer(test_texts, truncation=True, padding='max_length', max_length=emb_len)  # set the max_length to emb_len, so it has the same dimension as the model input\n",
    "token_input_test = np.asarray(test_texts['input_ids'])\n",
    "mask_input_test = np.asarray(test_texts['attention_mask'])\n",
    "\n",
    "loc_input_test = gloveAndPad(test_data['location'], config['loc_max_len'])\n",
    "\n",
    "print(token_input_test.shape)\n",
    "print(mask_input_test.shape)\n",
    "print(loc_input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "pred = model.predict(x={'tokens':token_input_test, 'masks':mask_input_test, 'locs':loc_input_test})\n",
    "pred = np.asarray(np.rint(pred), dtype=int)  # round to the closest integer, since we did a Sigmoid on the output layer\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data['id']\n",
    "submission['target'] = pred\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission8.csv', index=False)\n",
    "# Accuracy Score : 0.84737"
   ]
  },
  {
   "source": [
    "## Thank you for reading my notebook!\n",
    "- Please checkout my repository where I have different models(LSTM, CNN, ELMo, BERT etc.) tested on the same dataset.\n",
    "- https://github.com/MattYoon/NLP-Disaster_Tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}